# Docker Compose — one-command startup for both services.
#
# Usage:
#   docker compose up              — start both services
#   docker compose up --build      — rebuild and start
#   docker compose down            — stop everything
#
# For Ollama, add `network_mode: host` to mcp-server or use host.docker.internal.

services:
  rag-service:
    build: ./rag-service
    ports:
      - "8000:8000"
    environment:
      - RAG_HOST=0.0.0.0
      - RAG_PORT=8000
      - EMBEDDING_MODEL=all-MiniLM-L6-v2
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8000/health')"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s  # Model download on first run

  mcp-server:
    build: ./mcp-server
    ports:
      - "3000:3000"
    environment:
      - PORT=3000
      - RAG_SERVICE_URL=http://rag-service:8000
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - LLM_MODEL=${LLM_MODEL:-llama3.2}
      - LLM_BASE_URL=${LLM_BASE_URL:-http://host.docker.internal:11434/v1}
      - LLM_API_KEY=${LLM_API_KEY:-ollama}
    depends_on:
      rag-service:
        condition: service_healthy
